import os
import pdfplumber  # type: ignore
import fitz  # type: ignore
import re
import json
from collections import defaultdict

# ğŸ“Œ Yapay Zeka sektÃ¶rÃ¼ne Ã¶zel beceri listesi
# ğŸ“Œ AI becerilerini tek bir listeye Ã§evir
ALL_AI_SKILLS = {
    # 1. Programlama Dilleri ve Temel Beceriler
    "Programming_and_Fundamentals": [
        "Python", "R", "Julia", "Java", "C++", "JavaScript", "SQL", "Bash Scripting",
        "Data Structures", "Algorithms", "Version Control (Git)", "Debugging", "Code Optimization"
    ],

    # 2. Makine Ã–ÄŸrenmesi (Machine Learning)
    "Machine_Learning": [
        "Supervised Learning", "Unsupervised Learning", "Reinforcement Learning",
        "Semi-Supervised Learning", "Self-Supervised Learning", "Ensemble Methods",
        "Model Evaluation (Precision, Recall, F1, ROC-AUC)", "Hyperparameter Tuning (Grid Search, Random Search, Bayesian Optimization)",
        "Feature Engineering", "Feature Selection", "Dimensionality Reduction (PCA, t-SNE)",
        "Time Series Analysis", "Anomaly Detection", "Recommendation Systems", "Clustering (K-Means, DBSCAN)"
    ],

    # 3. Derin Ã–ÄŸrenme (Deep Learning)
    "Deep_Learning": [
        "Neural Networks (MLP, CNN, RNN, GAN)", "TensorFlow", "PyTorch", "Keras",
        "Transfer Learning", "Generative Models (GANs, VAEs)", "Optimization Techniques (Adam, SGD, RMSprop)",
        "Attention Mechanisms", "Transformers", "Explainable AI (XAI)", "Self-Supervised Learning",
        "Few-Shot Learning", "Meta-Learning", "Graph Neural Networks (GNNs)"
    ],

    # 4. DoÄŸal Dil Ä°ÅŸleme (Natural Language Processing - NLP)
    "Natural_Language_Processing": [
        "Text Preprocessing (Tokenization, Lemmatization, Stemming)", "Sentiment Analysis",
        "Named Entity Recognition (NER)", "Topic Modeling (LDA, NMF)", "Word Embeddings (Word2Vec, GloVe, FastText)",
        "Transformer Models (BERT, GPT, T5)", "Text Generation", "Machine Translation",
        "Speech Recognition (ASR)", "Chatbots", "Question Answering Systems", "Text Summarization",
        "Dialogue Systems", "Multilingual NLP", "Emotion Detection in Text"
    ],

    # 5. BilgisayarlÄ± GÃ¶rÃ¼ (Computer Vision)
    "Computer_Vision": [
        "Image Processing (OpenCV, PIL)", "Object Detection (YOLO, SSD, Faster R-CNN)",
        "Image Segmentation (U-Net, Mask R-CNN)", "Facial Recognition", "Optical Character Recognition (OCR)",
        "Pose Estimation", "Video Analysis", "Generative Models for Images (StyleGAN, CycleGAN)",
        "3D Vision", "Medical Image Analysis", "Scene Understanding", "Image Captioning"
    ],

    # 6. Veri Bilimi ve AnalitiÄŸi (Data Science and Analytics)
    "Data_Science_and_Analytics": [
        "Data Wrangling (Pandas, NumPy)", "Data Visualization (Matplotlib, Seaborn, Plotly)",
        "Statistical Analysis (Hypothesis Testing, A/B Testing)", "Big Data Tools (Hadoop, Spark)",
        "Data Pipelines (ETL, Airflow)", "Database Management (SQL, NoSQL)", "Cloud Platforms (AWS, GCP, Azure)",
        "Experimentation and Tracking (MLflow, Weights & Biases)", "Data Governance", "Data Cleaning",
        "Data Annotation", "Data Augmentation", "Feature Stores"
    ],

    # 7. AI AraÃ§larÄ± ve Ã‡erÃ§eveler (AI Tools and Frameworks)
    "AI_Tools_and_Frameworks": [
        "Scikit-Learn", "XGBoost", "LightGBM", "CatBoost", "TensorFlow", "PyTorch", "Keras",
        "Hugging Face Transformers", "OpenCV", "AutoML (H2O, AutoKeras)", "Model Deployment (Flask, FastAPI, Docker)",
        "ONNX", "MLOps Tools (Kubeflow, TFX)", "Ray", "Dask"
    ],

    # 8. Matematik ve Ä°statistik (Mathematics and Statistics)
    "Mathematics_and_Statistics": [
        "Linear Algebra", "Calculus", "Probability and Statistics", "Numerical Methods",
        "Optimization Theory", "Information Theory", "Bayesian Statistics", "Stochastic Processes",
        "Graph Theory", "Game Theory"
    ],

    # 9. Bulut ve DaÄŸÄ±tÄ±m (Cloud and Deployment)
    "Cloud_and_Deployment": [
        "AWS SageMaker", "Google AI Platform", "Azure ML", "Docker", "Kubernetes",
        "API Development (REST, GraphQL)", "Monitoring (Prometheus, Grafana)", "Serverless Computing",
        "Edge Computing", "Model Serving (TensorFlow Serving, Triton)", "CI/CD Pipelines"
    ],

    # 10. YumuÅŸak Beceriler (Soft Skills)
    "Soft_Skills": [
        "Problem-Solving", "Critical Thinking", "Communication", "Collaboration",
        "Project Management", "Time Management", "Leadership", "Adaptability",
        "Creativity", "Ethical Decision-Making"
    ],

    # 11. Yeni Nesil AI Becerileri (Emerging AI Skills)
    "Emerging_AI_Skills": [
        "Federated Learning", "Edge AI", "Quantum Machine Learning", "AI Ethics",
        "AI for Healthcare", "Multimodal AI", "AI in Robotics", "AI for Climate Science",
        "AI in Finance", "AI for Cybersecurity", "AI in Education", "AI for Autonomous Systems"
    ]
}

# ğŸ“Œ Skorlama fonksiyonu
def calculate_scores(skills, category_weights):
    scores = {}
    total_score = 0

    for category, skills_list in skills.items():
        category_score = 0
        for skill in skills_list:
            # Her bir beceri iÃ§in puan ekleyin (Ã¶rneÄŸin, her beceri 1 puan)
            category_score += 1

        # Kategori aÄŸÄ±rlÄ±ÄŸÄ± ile Ã§arpÄ±n
        category_score *= category_weights.get(category, 10)
        scores[category] = category_score
        total_score += category_score

    scores["total_score"] = total_score
    return scores

# ğŸ“Œ PDF'ten metin Ã§Ä±karma - pdfplumber yÃ¶ntemi
def extract_text_from_pdf(pdf_path):
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                extracted_text = page.extract_text()
                if extracted_text:
                    text += extracted_text + "\n"
    except Exception as e:
        print(f"âŒ Hata: {pdf_path} dosyasÄ± okunamadÄ±. Hata: {e}")
    return text

# ğŸ“Œ PDF'ten metin Ã§Ä±karma - PyMuPDF (fitz) yÃ¶ntemi
def extract_text_with_fitz(pdf_path):
    text = ""
    try:
        doc = fitz.open(pdf_path)
        text = "\n".join(page.get_text() for page in doc)
    except Exception as e:
        print(f"âŒ Hata: {pdf_path} dosyasÄ± okunamadÄ±. Hata: {e}")
    return text

# ğŸ“Œ Metin ve Karakter temizleme fonksiyonu
def clean_text(text):
    text = re.sub(r'\n+', '\n', text)  # Gereksiz boÅŸ satÄ±rlarÄ± kaldÄ±r
    text = re.sub(r'\s+', ' ', text)  # Fazla boÅŸluklarÄ± tek boÅŸluk yap
    text = text.replace("Ã¯Â¼â€‹", "")  # Ã–zel karakter dÃ¼zeltme
    text = text.replace("Ã‚", "")    # DiÄŸer hatalÄ± karakterler
    text = text.strip()
    return text

# ğŸ“Œ AI becerilerini metinden Ã§Ä±karma fonksiyonu
def extract_skills_from_text(text, skills_dict):
    skills = defaultdict(list)
    for skill_category, skills_list in skills_dict.items():
        for skill in skills_list:
            if re.search(rf"\b{re.escape(skill)}\b", text, re.IGNORECASE):
                skills[skill_category].append(skill)
    return skills

# ğŸ“Œ KullanÄ±cÄ± bilgilerini Ã§Ä±karma fonksiyonu
def extract_user_info_from_text(text):
    name = re.search(r"\b[A-Z][a-zA-Z'-]+\s+[A-Z][a-zA-Z'-]+\b", text)
    email = re.search(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", text)
    phone = re.search(r"\+\d{1,4}[\s-]?\(?\d{1,4}\)?[\s-]?\d{2,4}[\s-]?\d{2,4}[\s-]?\d{2,4}(?![\d-])", text)  # Telefon numarasÄ±
    linkedin = re.search(r"(?:https?:\/\/)?(?:www\.)?linkedin\.com\/[a-zA-Z0-9-\._%]+", text)  # LinkedIn profili
    github = re.search(r"(?:https?:\/\/)?(?:www\.)?github\.com\/[a-zA-Z0-9-\._%]+", text)  # GitHub profili

    return {
        "name": name.group(0) if name else "N/A",
        "email": email.group(0) if email else "N/A",
        "phone": phone.group(0) if phone else "N/A",
        "linkedin": linkedin.group(0) if linkedin else "N/A",
        "github": github.group(0) if github else "N/A",
    }

# ğŸ“Œ Pozisyona gÃ¶re aÄŸÄ±rlÄ±klandÄ±rma
def get_category_weights(target_category):
    # VarsayÄ±lan aÄŸÄ±rlÄ±klar
    category_weights = {
        "Machine_Learning": 10,
        "Deep_Learning": 10,
        "Data_Science_and_Analytics": 10,
        "Natural_Language_Processing": 10,
        "Computer_Vision": 10,
        "AI_Tools_and_Frameworks": 10,
        "Programming_and_Fundamentals": 5,
        "Mathematics_and_Statistics": 5,
        "Cloud_and_Deployment": 5,
        "Soft_Skills": 2.5,
        "Emerging_AI_Skills": 2.5
    }

    # Hedef kategori iÃ§in aÄŸÄ±rlÄ±ÄŸÄ± artÄ±r
    if target_category in category_weights:
        category_weights[target_category] = 15  # Hedef kategoriye 15 aÄŸÄ±rlÄ±k ver

    return category_weights

# ğŸ“Œ PDF'den tÃ¼m bilgileri Ã§Ä±karma
def extract_info_from_pdf(text, skills_dict, target_category):
    skills = extract_skills_from_text(text, skills_dict)
    user_info = extract_user_info_from_text(text)

    # Hedef kategoriye gÃ¶re aÄŸÄ±rlÄ±klandÄ±rma
    category_weights = get_category_weights(target_category)

    # SkorlarÄ± hesapla
    scores = calculate_scores(skills, category_weights)

    # KullanÄ±cÄ± bilgilerini ve becerileri birleÅŸtir
    user_info["skills"] = skills
    user_info["scores"] = scores
    user_info["target_category"] = target_category  # Hedef kategoriyi de kaydet

    return user_info

# ğŸ“Œ Ã–zgeÃ§miÅŸleri iÅŸleme fonksiyonu
def process_resumes_with_ai_skills(folder_path, skills_dict, target_category, method="plumber"):
    processed_resumes = {}

    for file_name in os.listdir(folder_path):
        if file_name.endswith(".pdf"):
            print(f"ğŸ” Ä°ÅŸleniyor: {file_name}")
            file_path = os.path.join(folder_path, file_name)
            text = ""
            if method == "plumber":
                text = extract_text_from_pdf(file_path)
            elif method == "fitz":
                text = extract_text_with_fitz(file_path)

            user_info = extract_info_from_pdf(text, skills_dict, target_category)
            processed_resumes[file_name] = user_info

    return processed_resumes

# ğŸ“Œ JSON olarak kaydetme fonksiyonu
def save_results_to_json(results, output_file="processed_ai_resumes.json"):
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=4)

# ğŸ“Œ KullanÄ±cÄ±dan hedef kategoriyi al
def get_target_category_from_user():
    print("LÃ¼tfen aÅŸaÄŸÄ±daki kategorilerden birini seÃ§in:")
    for i, category in enumerate(ALL_AI_SKILLS.keys(), 1):
        print(f"{i}. {category}")
    choice = int(input("SeÃ§iminiz (numara): "))
    target_category = list(ALL_AI_SKILLS.keys())[choice - 1]
    return target_category

# ğŸ“Œ Ana iÅŸlem
if __name__ == "__main__":
    folder_path = "resumes/"  # Ã–zgeÃ§miÅŸlerin bulunduÄŸu klasÃ¶r

    # KullanÄ±cÄ±dan hedef kategoriyi al
    target_category = get_target_category_from_user()
    print(f"SeÃ§ilen kategori: {target_category}")

    # KullanÄ±cÄ±dan kaÃ§ aday gÃ¶rmek istediÄŸini al
    try:
        num_candidates = int(input("KaÃ§ aday gÃ¶rmek istersiniz? (Ã–rneÄŸin: 5): "))
        if num_candidates <= 0:
            raise ValueError("LÃ¼tfen pozitif bir sayÄ± girin.")
    except ValueError as e:
        print(f"âŒ Hata: GeÃ§ersiz giriÅŸ. {e}")
        exit()

    # Ã–zgeÃ§miÅŸleri iÅŸle
    processed_resumes = process_resumes_with_ai_skills(folder_path, ALL_AI_SKILLS, target_category, method="plumber")

    # JSON olarak kaydet
    save_results_to_json(processed_resumes, output_file="processed_ai_resumes.json")

    # SonuÃ§larÄ± sÄ±rala ve gÃ¶ster
    sorted_resumes = sorted(processed_resumes.items(), key=lambda x: x[1]["scores"]["total_score"], reverse=True)
    print(f"\nğŸ” En uygun {num_candidates} aday:")
    for i, (file_name, user_info) in enumerate(sorted_resumes[:num_candidates], 1):
        print(f"{i}. {user_info['name']} - Toplam Puan: {user_info['scores']['total_score']}")

    print(f"\nâœ… {len(processed_resumes)} adet Ã¶zgeÃ§miÅŸ iÅŸlendi ve 'processed_ai_resumes.json' dosyasÄ±na kaydedildi.")